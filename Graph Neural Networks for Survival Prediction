{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Graph Neural Networks for Survival Prediction: A Novel Biostatistical Approach to the Titanic Dataset\n\nThe Titanic dataset is a classic dataset for binary classification (predicting survival: 0 or 1) and has direct relevance to biostatistics, particularly in the domain of survival analysis. Biostatistics applies statistical methods to biological and medical data, often focusing on outcomes like survival, disease progression, or treatment efficacy. Here’s why this project aligns with biostatistical goals:\n\n- Survival Analysis Context: The Titanic dataset involves predicting whether passengers survived the disaster based on features like age, sex, and passenger class. This mirrors survival analysis tasks in biostatistics, where the outcome is a binary event (e.g., survival vs. death). The dataset has been referenced in biostatistical contexts, such as by the Vanderbilt University Department of Biostatistics.\n- Population-Level Insights: By using a GNN, the project models passengers as nodes in a graph, with edges representing similarity in characteristics. This approach captures relational patterns (e.g., whether passengers with similar profiles had correlated survival outcomes), which is valuable in biostatistics for understanding population dynamics, such as in epidemiological studies or clinical trials.\n- Innovative Methodology: Traditional biostatistical models for survival analysis (e.g., logistic regression, Cox proportional hazards) treat observations as independent. The GNN’s graph-based approach introduces a novel perspective by modeling dependencies between individuals, potentially revealing insights into group-level factors affecting survival (e.g., access to lifeboats based on class or gender).","metadata":{}},{"cell_type":"markdown","source":"## 1. Installing Dependencies\nInstalls the <span style=\"color:orange\">torch-geometric</span> library, which is required for implementing the GCN model. This library is not pre-installed in Kaggle notebooks, so it must be installed at runtime.\n- <span style=\"color:orange\">torch-geometric</span> provides tools for graph-based deep learning, including the <span style=\"color:orange\">GCNConv</span> layer used in the model.\n\nEnables the use of advanced graph neural network techniques, which are central to the project’s unique approach.","metadata":{}},{"cell_type":"code","source":"!pip install torch-geometric","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:33.612720Z","iopub.execute_input":"2025-05-11T23:45:33.613255Z","iopub.status.idle":"2025-05-11T23:45:36.633118Z","shell.execute_reply.started":"2025-05-11T23:45:33.613228Z","shell.execute_reply":"2025-05-11T23:45:36.632202Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.16)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.19.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch-geometric) (2024.2.0)\n","output_type":"stream"}],"execution_count":300},{"cell_type":"markdown","source":"## 2. Importing Libraries\n- Data Manipulation:\n  - <span style=\"color:orange\">pandas (pd):</span> Handles the Titanic dataset as a DataFrame for loading and preprocessing.\n  - <span style=\"color:orange\">numpy (np):</span> Supports numerical operations, such as array manipulation.\n- Preprocessing:\n  - <span style=\"color:orange\">sklearn.preprocessing.StandardScaler:</span> Standardizes features to have zero mean and unit variance, crucial for distance-based graph construction.\n  - <span style=\"color:orange\">sklearn.model_selection.train_test_split:</span> Splits data into training and validation sets.\n  - <span style=\"color:orange\">sklearn.metrics.pairwise_distances:</span> Computes pairwise distances between passengers for graph construction.\n- Deep Learning:\n  - <span style=\"color:orange\">torch:</span> PyTorch library for tensor operations and neural network implementation.\n  - <span style=\"color:orange\">torch.nn.functional (F):</span> Provides activation functions (e.g., ReLU) and loss functions.\n- Graph Neural Networks:\n  - <span style=\"color:orange\">torch_geometric.data.Data:</span> Creates a graph data object for GNN processing.\n  - <span style=\"color:orange\">torch_geometric.nn.GCNConv:</span> Implements graph convolutional layers for the GCN model.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import pairwise_distances\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GCNConv\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:36.634831Z","iopub.execute_input":"2025-05-11T23:45:36.635044Z","iopub.status.idle":"2025-05-11T23:45:36.640388Z","shell.execute_reply.started":"2025-05-11T23:45:36.635022Z","shell.execute_reply":"2025-05-11T23:45:36.639806Z"}},"outputs":[],"execution_count":301},{"cell_type":"markdown","source":"## 3. Loading the Dataset\nThe dataset is stored in <span style=\"color:orange\">/kaggle/input/titanic/train.csv</span>, as added via Kaggle’s data interface.\n- <span style=\"color:orange\">pandas.read_csv</span> reads the CSV file into a DataFrame with 891 rows and 12 columns: <span style=\"color:orange\">PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, and Embarked.</span>\n\nThe <span style=\"color:orange\">Survived</span> column is the target variable (0 = did not survive, 1 = survived), and other columns are potential features.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/titanic/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:36.641109Z","iopub.execute_input":"2025-05-11T23:45:36.641292Z","iopub.status.idle":"2025-05-11T23:45:36.668620Z","shell.execute_reply.started":"2025-05-11T23:45:36.641277Z","shell.execute_reply":"2025-05-11T23:45:36.667920Z"}},"outputs":[],"execution_count":302},{"cell_type":"markdown","source":"## 4. Handling Missing Values\n- Embarked:\n    - <span style=\"color:orange\">Embarked</span> (port of embarkation: C, Q, S) has 2 missing values.\n    - <span style=\"color:orange\">mode()[0]</span> finds the most common value (‘S’).\n    - <span style=\"color:orange\">fillna</span> replaces missing values with ‘S’.\n- Age:\n    - <span style=\"color:orange\">Age</span> has 177 missing values (about 20% of the dataset).\n    - <span style=\"color:orange\">mean()</span> computes the average age (approximately 29.7 years).\n    - <span style=\"color:orange\">fillna</span> replaces missing values with the mean age.\n- <span style=\"color:orange\">inplace=True</span> modifies the DataFrame directly to avoid creating a copy.","metadata":{}},{"cell_type":"code","source":"most_common_embarked = train['Embarked'].mode()[0]\n# train['Embarked'].fillna(most_common_embarked, inplace=True)\ntrain['Embarked'] = train['Embarked'].fillna(most_common_embarked)\nmean_age = train['Age'].mean()\n# train['Age'].fillna(mean_age, inplace=True)\ntrain['Age'] = train['Age'].fillna(mean_age)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:36.670768Z","iopub.execute_input":"2025-05-11T23:45:36.671233Z","iopub.status.idle":"2025-05-11T23:45:36.677706Z","shell.execute_reply.started":"2025-05-11T23:45:36.671208Z","shell.execute_reply":"2025-05-11T23:45:36.676953Z"}},"outputs":[],"execution_count":303},{"cell_type":"markdown","source":"## 5. Encoding Categorical Variables\n- Sex:\n    - <span style=\"color:orange\">Sex</span> (male, female) is binary and mapped to 0 (male) or 1 (female) using <span style=\"color:orange\">map</span>.\n- Embarked:\n    - <span style=\"color:orange\">Embarked</span> (C, Q, S) is one-hot encoded using <span style=\"color:orange\">pd.get_dummies</span>, creating three binary columns: <span style=\"color:orange\">Embarked_C, Embarked_Q, Embarked_S</span>.\n    - The original <span style=\"color:orange\">Embarked</span> column is dropped.\n- Pclass:\n    - <span style=\"color:orange\">Pclass</span> (1, 2, 3) is one-hot encoded into <span style=\"color:orange\">Pclass_1, Pclass_2, Pclass_3</span>.\n    - The original <span style=\"color:orange\">Pclass</span> column is dropped.\n    - <span style=\"color:orange\">pd.concat</span> combines the new dummy columns with the DataFrame, and <span style=\"color:orange\">axis=1</span> ensures concatenation along columns.\n\nMachine learning models, including GNNs, require numerical inputs. One-hot encoding preserves the categorical nature of <span style=\"color:orange\">Embarked</span> and <span style=\"color:orange\">Pclass</span> without implying ordinality, while binary encoding for <span style=\"color:orange\">Sex</span> is efficient. This step prepares the data for feature standardization and graph construction.","metadata":{}},{"cell_type":"code","source":"train['Sex'] = train['Sex'].map({'male': 0, 'female': 1})\nembarked_dummies = pd.get_dummies(train['Embarked'], prefix='Embarked')\ntrain = pd.concat([train, embarked_dummies], axis=1)\ntrain.drop('Embarked', axis=1, inplace=True)\npclass_dummies = pd.get_dummies(train['Pclass'], prefix='Pclass')\ntrain = pd.concat([train, pclass_dummies], axis=1)\ntrain.drop('Pclass', axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:36.678429Z","iopub.execute_input":"2025-05-11T23:45:36.678644Z","iopub.status.idle":"2025-05-11T23:45:36.703345Z","shell.execute_reply.started":"2025-05-11T23:45:36.678628Z","shell.execute_reply":"2025-05-11T23:45:36.702807Z"}},"outputs":[],"execution_count":304},{"cell_type":"markdown","source":"## 6. Dropping Irrelevant Columns\nRemoves columns that are not useful for modeling due to irrelevance or high missingness.\n- PassengerId: A unique identifier with no predictive value.\n- Name: Contains passenger names, which are not directly useful without feature engineering (e.g., extracting titles).\n- Ticket: Ticket numbers are mostly unique and lack clear predictive patterns.\n- Cabin: Has 687 missing values (77% missing) and is too sparse to use effectively.\n- <span style=\"color:orange\">axis=1</span> specifies column-wise dropping, and <span style=\"color:orange\">inplace=True</span> modifies the DataFrame.\n\nSimplifies the dataset to focus on predictive features (<span style=\"color:orange\">Age, SibSp, Parch, Fare, Sex, Pclass_1, Pclass_2, Pclass_3, Embarked_C, Embarked_Q, Embarked_S</span>), reducing noise and computational complexity.","metadata":{}},{"cell_type":"code","source":"train.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:36.704058Z","iopub.execute_input":"2025-05-11T23:45:36.704347Z","iopub.status.idle":"2025-05-11T23:45:36.726381Z","shell.execute_reply.started":"2025-05-11T23:45:36.704321Z","shell.execute_reply":"2025-05-11T23:45:36.725650Z"}},"outputs":[],"execution_count":305},{"cell_type":"markdown","source":"## 7. Defining Features and Target\nSeparates the feature matrix (<span style=\"color:orange\">X</span>) from the target variable (<span style=\"color:orange\">y</span>).\n- <span style=\"color:orange\">X</span>: Contains all columns except <span style=\"color:orange\">Survived</span>, resulting in an 891 x 11 matrix (11 features).\n- <span style=\"color:orange\">y</span>: The <span style=\"color:orange\">Survived</span> column, a binary vector (0 or 1) with 891 entries.\n- <span style=\"color:orange\">drop(axis=1)</span> removes the <span style=\"color:orange\">Survived</span> column from the DataFrame to create <span style=\"color:orange\">X</span>.\n\nClearly defines the inputs (<span style=\"color:orange\">X</span>) and outputs (<span style=\"color:orange\">y</span>) for the machine learning model, a standard step in supervised learning tasks like survival prediction.","metadata":{}},{"cell_type":"code","source":"X = train.drop('Survived', axis=1)\ny = train['Survived']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:36.727611Z","iopub.execute_input":"2025-05-11T23:45:36.727928Z","iopub.status.idle":"2025-05-11T23:45:36.743937Z","shell.execute_reply.started":"2025-05-11T23:45:36.727904Z","shell.execute_reply":"2025-05-11T23:45:36.743406Z"}},"outputs":[],"execution_count":306},{"cell_type":"markdown","source":"## 8. Standardizing Features\nStandardizes the features to have zero mean and unit variance, ensuring consistent scales for distance calculations.\n- <span style=\"color:orange\">StandardScaler</span> computes the mean and standard deviation for each feature and transforms the data using <span style=\"color:orange\">(x - mean) / std</span>.\n- <span style=\"color:orange\">fit_transform</span> fits the scaler to <span style=\"color:orange\">X</span> and transforms it in one step, producing <span style=\"color:orange\">X_scaled</span> (an 891 x 11 NumPy array).\n- Standardization is critical because features like <span style=\"color:orange\">Age</span> (range: 0-80) and <span style=\"color:orange\">Fare</span> (range: 0-512) have different scales, which could skew distance-based graph construction.\n\nEnsures that all features contribute equally to the Euclidean distance calculations used to build the graph, improving the quality of the GNN’s relational modeling.","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:36.744611Z","iopub.execute_input":"2025-05-11T23:45:36.744852Z","iopub.status.idle":"2025-05-11T23:45:36.766006Z","shell.execute_reply.started":"2025-05-11T23:45:36.744837Z","shell.execute_reply":"2025-05-11T23:45:36.765431Z"}},"outputs":[],"execution_count":307},{"cell_type":"markdown","source":"## 9. Splitting Train and Validation Sets\nSplits the dataset into 80% training and 20% validation sets to evaluate model performance.\n- <span style=\"color:orange\">train_test_split</span> splits the indices (0 to 890) into training (<span style=\"color:orange\">train_idx</span>) and validation (<span style=\"color:orange\">val_idx</span>) sets.\n- <span style=\"color:orange\">test_size=0.2</span> allocates 20% (179 samples) for validation and 80% (712 samples) for training.\n- <span style=\"color:orange\">random_state=42</span> ensures reproducibility.\n- <span style=\"color:orange\">stratify=y</span> maintains the proportion of <span style=\"color:orange\">Survived</span> classes (approximately 38% survived) in both sets.\n\nAllows the model to train on a subset of the data and evaluate generalization on unseen data, a key practice in machine learning to prevent overfitting.","metadata":{}},{"cell_type":"code","source":"train_idx, val_idx = train_test_split(range(len(X)), test_size=0.2, random_state=42, stratify=y)\nprint(f\"Train set size: {len(train_idx)}, Validation set size: {len(val_idx)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:36.766818Z","iopub.execute_input":"2025-05-11T23:45:36.767097Z","iopub.status.idle":"2025-05-11T23:45:36.782739Z","shell.execute_reply.started":"2025-05-11T23:45:36.767066Z","shell.execute_reply":"2025-05-11T23:45:36.782242Z"}},"outputs":[{"name":"stdout","text":"Train set size: 712, Validation set size: 179\n","output_type":"stream"}],"execution_count":308},{"cell_type":"markdown","source":"## 10. Constructing the Graph\nBuilds a graph where passengers are nodes, and edges connect each passenger to their 5 most similar peers based on feature similarity.\n- Distance Calculation:\n    - <span style=\"color:orange\">pairwise_distances(X_scaled)</span> computes Euclidean distances between all pairs of passengers, producing an 891 x 891 matrix.\n- K-Nearest Neighbors:\n    - <span style=\"color:orange\">np.argsort(distances, axis=1)</span> sorts distances for each passenger.\n    - <span style=\"color:orange\">[:, 1:k+1]</span> selects the indices of the 5 closest neighbors (excluding the passenger themselves, hence <span style=\"color:orange\">1:k+1</span>).\n    - <span style=\"color:orange\">knn_indices</span> is an 891 x 5 array of neighbor indices.\n- Edge List:\n    - The loop creates edges by pairing each passenger (<span style=\"color:orange\">i</span>) with their neighbors (<span style=\"color:orange\">j</span>).\n    - Both <span style=\"color:orange\">[i, j]</span> and <span style=\"color:orange\">[j, i]</span> are added to make the graph undirected.\n    - <span style=\"color:orange\">edges</span> is a list of edge pairs.\n- Remove Duplicates:\n    - <span style=\"color:orange\">np.unique(np.array(edges), axis=0)</span> removes duplicate edges (e.g., [i, j] and [j, i] are equivalent in an undirected graph).\n\nThe graph structure is the core of the GNN, enabling the model to learn from relational patterns (e.g., passengers with similar ages or classes may have correlated survival outcomes). This relational approach is novel for the Titanic dataset and aligns with biostatistical interest in population-level interactions.","metadata":{}},{"cell_type":"code","source":"distances = pairwise_distances(X_scaled)\nk = 5\nknn_indices = np.argsort(distances, axis=1)[:, 1:k+1]\nedges = []\nfor i in range(len(X)):\n    for j in knn_indices[i]:\n        edges.append([i, j])\n        edges.append([j, i])\nedges = np.unique(np.array(edges), axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:36.784770Z","iopub.execute_input":"2025-05-11T23:45:36.784969Z","iopub.status.idle":"2025-05-11T23:45:36.845353Z","shell.execute_reply.started":"2025-05-11T23:45:36.784954Z","shell.execute_reply":"2025-05-11T23:45:36.844818Z"}},"outputs":[],"execution_count":309},{"cell_type":"markdown","source":"## 11. Creating the PyTorch Geometric Data Object\nConverts the data into a <span style=\"color:orange\">Data</span> object compatible with PyTorch Geometric for GNN training.\n- Node Features:\n    - <span style=\"color:orange\">x = torch.tensor(X_scaled, dtype=torch.float)</span> converts the standardized features (891 x 11) to a PyTorch tensor.\n- Edge Index:\n    - <span style=\"color:orange\">edge_index = torch.tensor(edges.T, dtype=torch.long)</span> converts the edge list to a 2 x M tensor, where M is the number of unique edges. The first row contains source nodes, and the second row contains target nodes.\n- Labels:\n    - <span style=\"color:orange\">y_torch = torch.tensor(y.values, dtype=torch.long)</span> converts the <span style=\"color:orange\">Survived</span> labels to a tensor of integers (0 or 1).\n- Masks:\n    - <span style=\"color:orange\">train_mask</span> and <span style=\"color:orange\">val_mask</span> are boolean tensors (length 891) initialized to <span style=\"color:orange\">False</span>.\n    - <span style=\"color:orange\">train_mask[train_idx] = True</span> sets <span style=\"color:orange\">True</span> for training indices.\n    - <span style=\"color:orange\">val_mask[val_idx] = True</span> sets <span style=\"color:orange\">True</span> for validation indices.\n- Data Object:\n    - <span style=\"color:orange\">Data</span> combines <span style=\"color:orange\">x, edge_index, y, train_mask,</span> and <span style=\"color:orange\">val_mask</span> into a single object for GNN processing.\n\nThe <span style=\"color:orange\">Data</span> object encapsulates the graph structure and data needed for the GCN, enabling efficient graph-based learning. The masks ensure the model trains and evaluates on the correct subsets.","metadata":{}},{"cell_type":"code","source":"x = torch.tensor(X_scaled, dtype=torch.float)\nedge_index = torch.tensor(edges.T, dtype=torch.long)\ny_torch = torch.tensor(y.values, dtype=torch.long)\ntrain_mask = torch.zeros(len(X), dtype=torch.bool)\ntrain_mask[train_idx] = True\nval_mask = torch.zeros(len(X), dtype=torch.bool)\nval_mask[val_idx] = True\ndata = Data(x=x, edge_index=edge_index, y=y_torch, train_mask=train_mask, val_mask=val_mask)\nprint(f\"Train mask sum: {data.train_mask.sum().item()}, Val mask sum: {data.val_mask.sum().item()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:36.846030Z","iopub.execute_input":"2025-05-11T23:45:36.846242Z","iopub.status.idle":"2025-05-11T23:45:36.852809Z","shell.execute_reply.started":"2025-05-11T23:45:36.846219Z","shell.execute_reply":"2025-05-11T23:45:36.852242Z"}},"outputs":[{"name":"stdout","text":"Train mask sum: 712, Val mask sum: 179\n","output_type":"stream"}],"execution_count":310},{"cell_type":"markdown","source":"## 12. Defining the GCN Model\nDefines a two-layer Graph Convolutional Network to predict survival probabilities.\n- Class Definition:\n    - <span style=\"color:orange\">GCN</span> inherits from <span style=\"color:orange\">torch.nn.Module</span>, the base class for PyTorch models.\n- Initialization:\n    - <span style=\"color:orange\">input_dim</span>: Number of input features (11).\n    - <span style=\"color:orange\">hidden_dim</span>: Size of the hidden layer (16).\n    - <span style=\"color:orange\">output_dim</span>: Number of output classes (2: survive or not).\n    - <span style=\"color:orange\">conv1</span>: First GCN layer, mapping 11 features to 16 dimensions.\n    - <span style=\"color:orange\">conv2</span>: Second GCN layer, mapping 16 dimensions to 2.\n- Forward Pass:\n    - <span style=\"color:orange\">x, edge_index = data.x, data.edge_index</span>: Extracts node features and edges.\n    - <span style=\"color:orange\">conv1</span>: Applies graph convolution, aggregating information from neighboring nodes.\n    - <span style=\"color:orange\">F.relu</span>: Applies ReLU activation to introduce non-linearity.\n    - <span style=\"color:orange\">F.dropout(p=0.5, training=self.training)</span>: Randomly zeros 50% of the features during training to prevent overfitting.\n    - <span style=\"color:orange\">conv2</span>: Second graph convolution produces logits for each node.\n    - <span style=\"color:orange\">F.log_softmax(x, dim=1)</span>: Converts logits to log-probabilities for classification.\n\nThe GCN leverages the graph structure to learn features that combine individual passenger data with information from similar passengers, making it suitable for capturing relational patterns in survival outcomes.","metadata":{}},{"cell_type":"code","source":"class GCN(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(input_dim, hidden_dim)\n        self.conv2 = GCNConv(hidden_dim, output_dim)\n    \n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.conv2(x, edge_index)\n        return F.log_softmax(x, dim=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:36.853468Z","iopub.execute_input":"2025-05-11T23:45:36.853718Z","iopub.status.idle":"2025-05-11T23:45:36.869678Z","shell.execute_reply.started":"2025-05-11T23:45:36.853697Z","shell.execute_reply":"2025-05-11T23:45:36.869195Z"}},"outputs":[],"execution_count":311},{"cell_type":"markdown","source":"## 13. Initializing Model and Optimizer\nSets up the GCN model, moves it to the appropriate device (GPU or CPU), and configures the optimizer.\n- Device:\n    - <span style=\"color:orange\">device</span> checks for GPU availability (Kaggle provides GPU P100 or T4). If unavailable, it defaults to CPU.\n- Model:\n    - <span style=\"color:orange\">GCN</span> is instantiated with <span style=\"color:orange\">input_dim=11</span> (number of features), <span style=\"color:orange\">hidden_dim=16</span> (arbitrary choice for hidden layer size), and <span style=\"color:orange\">output_dim=2</span> (binary classification).\n    - <span style=\"color:orange\">to(device)</span> moves the model to the GPU or CPU.\n- Data:\n    - <span style=\"color:orange\">data.to(device)</span> moves the <span style=\"color:orange\">Data</span> object (features, edges, labels, masks) to the same device as the model.\n- Optimizer:\n    - <span style=\"color:orange\">Adam</span> optimizer is used with a learning rate (<span style=\"color:orange\">lr</span>) of 0.01 and weight decay (weight_decay) of 5e-4 for L2 regularization to prevent overfitting.\n    - <span style=\"color:orange\">model.parameters()</span> provides the model’s trainable weights.\n\nInitializes the training setup, ensuring compatibility between the model and data. The use of GPU accelerates training, and Adam is a robust optimizer for deep learning tasks.","metadata":{}},{"cell_type":"code","source":"# Initialize model and optimizer\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = GCN(input_dim=X_scaled.shape[1], hidden_dim=16, output_dim=2).to(device)\ndata = data.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:36.870267Z","iopub.execute_input":"2025-05-11T23:45:36.870421Z","iopub.status.idle":"2025-05-11T23:45:36.897790Z","shell.execute_reply.started":"2025-05-11T23:45:36.870409Z","shell.execute_reply":"2025-05-11T23:45:36.897088Z"}},"outputs":[],"execution_count":312},{"cell_type":"code","source":"# Lists to store losses for plotting\ntrain_losses = []\nval_losses = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:36.898520Z","iopub.execute_input":"2025-05-11T23:45:36.898729Z","iopub.status.idle":"2025-05-11T23:45:36.920100Z","shell.execute_reply.started":"2025-05-11T23:45:36.898713Z","shell.execute_reply":"2025-05-11T23:45:36.919565Z"}},"outputs":[],"execution_count":313},{"cell_type":"markdown","source":"## 14. Training Loop\nTrains the GCN model for 100 epochs, updating weights to minimize the loss on the training set.\n- Loop Setup:\n    - <span style=\"color:orange\">num_epochs = 100</span>: Trains for 100 iterations over the dataset.\n- Training Mode:\n    - <span style=\"color:orange\">model.train()</span> enables training-specific behaviors like dropout.\n- Forward Pass:\n    - <span style=\"color:orange\">optimizer.zero_grad()</span> clears previous gradients.\n    - <span style=\"color:orange\">out = model(data)</span> computes log-probabilities for all nodes.\n- Loss Calculation:\n    - <span style=\"color:orange\">F.nll_loss(out[data.train_mask], data.y[data.train_mask])</span> computes the negative log-likelihood loss for training nodes only, comparing predicted log-probabilities to true labels.\n- Backpropagation:\n    - <span style=\"color:orange\">loss.backward()</span> computes gradients.\n    - <span style=\"color:orange\">optimizer.step()</span> updates model weights using Adam.\n\nThe training loop optimizes the GCN to predict survival accurately, leveraging the graph structure to learn relational patterns. The use of <span style=\"color:orange\">train_mask</span> ensures only training nodes contribute to the loss.","metadata":{}},{"cell_type":"markdown","source":"## 15. Validation\nEvaluates the model on the validation set after each epoch, reporting loss and accuracy.\n- Evaluation Mode:\n    - <span style=\"color:orange\">model.eval()</span> disables dropout and other training-specific behaviors.\n    - <span style=\"color:orange\">with torch.no_grad()</span> prevents gradient computation for efficiency.\n- Validation Forward Pass:\n    - <span style=\"color:orange\">val_out = model(data)</span> computes predictions for all nodes.\n- Validation Loss:\n    - <span style=\"color:orange\">F.nll_loss(val_out[data.val_mask], data.y[data.val_mask])</span> computes the loss for validation nodes.\n- Accuracy:\n    - <span style=\"color:orange\">val_out.max(dim=1)</span> returns the predicted class (0 or 1) for each node.\n    - <span style=\"color:orange\">pred[data.val_mask].eq(data.y[data.val_mask])</span> compares predictions to true labels for validation nodes.\n    - <span style=\"color:orange\">sum().item()</span> counts correct predictions.\n    - <span style=\"color:orange\">acc</span> is the proportion of correct predictions (correct / 179).\n- Logging:\n    - <span style=\"color:orange\">print</span> outputs the epoch number, training loss, validation loss, and validation accuracy.\n\nValidation metrics assess the model’s generalization to unseen data, crucial for ensuring the GNN is not overfitting. The printed metrics help monitor training progress and model performance.","metadata":{}},{"cell_type":"code","source":"num_epochs = 100\nval_block_count = 0\nfor epoch in range(num_epochs):\n    model.train()\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n    train_losses.append(loss.item())\n    # Validation\n    model.eval()\n    with torch.no_grad():\n        if data.val_mask.sum() == 0:\n            raise ValueError(\"Validation mask is empty. Check train_test_split or val_mask initialization.\")\n        try:\n            val_out = model(data)\n            # Debug tensor shapes\n            print(f\"Epoch {epoch+1}, val_out shape: {val_out.shape}, val_mask shape: {data.val_mask.shape}, y shape: {data.y.shape}\")\n            if val_out[data.val_mask].shape[0] != data.y[data.val_mask].shape[0]:\n                raise ValueError(f\"Shape mismatch: val_out[data.val_mask] {val_out[data.val_mask].shape}, y[data.val_mask] {data.y[data.val_mask].shape}\")\n            val_loss = F.nll_loss(val_out[data.val_mask], data.y[data.val_mask])\n            val_losses.append(val_loss.item())\n            val_block_count += 1\n            print(f\"Epoch {epoch+1}, Val Loss: {val_loss.item():.4f}, Val Losses Length: {len(val_losses)}\")\n            _, pred = val_out.max(dim=1)\n            correct = pred[data.val_mask].eq(data.y[data.val_mask]).sum().item()\n            acc = correct / data.val_mask.sum().item() if data.val_mask.sum().item() > 0 else 0.0\n            print(f\"Epoch {epoch+1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, Val Acc: {acc:.4f}\")\n        except Exception as e:\n            print(f\"Validation error in epoch {epoch+1}: {str(e)}\")\n            continue","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:36.920785Z","iopub.execute_input":"2025-05-11T23:45:36.920984Z","iopub.status.idle":"2025-05-11T23:45:37.454448Z","shell.execute_reply.started":"2025-05-11T23:45:36.920969Z","shell.execute_reply":"2025-05-11T23:45:37.453868Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 1, Val Loss: 0.6118, Val Losses Length: 1\nEpoch 1, Train Loss: 0.6170, Val Loss: 0.6118, Val Acc: 0.6592\nEpoch 2, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 2, Val Loss: 0.5908, Val Losses Length: 2\nEpoch 2, Train Loss: 0.5786, Val Loss: 0.5908, Val Acc: 0.7039\nEpoch 3, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 3, Val Loss: 0.5719, Val Losses Length: 3\nEpoch 3, Train Loss: 0.5804, Val Loss: 0.5719, Val Acc: 0.7095\nEpoch 4, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 4, Val Loss: 0.5555, Val Losses Length: 4\nEpoch 4, Train Loss: 0.5587, Val Loss: 0.5555, Val Acc: 0.7263\nEpoch 5, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 5, Val Loss: 0.5416, Val Losses Length: 5\nEpoch 5, Train Loss: 0.5463, Val Loss: 0.5416, Val Acc: 0.7430\nEpoch 6, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 6, Val Loss: 0.5297, Val Losses Length: 6\nEpoch 6, Train Loss: 0.5351, Val Loss: 0.5297, Val Acc: 0.7598\nEpoch 7, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 7, Val Loss: 0.5196, Val Losses Length: 7\nEpoch 7, Train Loss: 0.5036, Val Loss: 0.5196, Val Acc: 0.7598\nEpoch 8, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 8, Val Loss: 0.5110, Val Losses Length: 8\nEpoch 8, Train Loss: 0.4953, Val Loss: 0.5110, Val Acc: 0.7709\nEpoch 9, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 9, Val Loss: 0.5041, Val Losses Length: 9\nEpoch 9, Train Loss: 0.4838, Val Loss: 0.5041, Val Acc: 0.7709\nEpoch 10, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 10, Val Loss: 0.4983, Val Losses Length: 10\nEpoch 10, Train Loss: 0.4849, Val Loss: 0.4983, Val Acc: 0.7654\nEpoch 11, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 11, Val Loss: 0.4935, Val Losses Length: 11\nEpoch 11, Train Loss: 0.4703, Val Loss: 0.4935, Val Acc: 0.7542\nEpoch 12, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 12, Val Loss: 0.4895, Val Losses Length: 12\nEpoch 12, Train Loss: 0.4658, Val Loss: 0.4895, Val Acc: 0.7542\nEpoch 13, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 13, Val Loss: 0.4860, Val Losses Length: 13\nEpoch 13, Train Loss: 0.4641, Val Loss: 0.4860, Val Acc: 0.7654\nEpoch 14, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 14, Val Loss: 0.4829, Val Losses Length: 14\nEpoch 14, Train Loss: 0.4548, Val Loss: 0.4829, Val Acc: 0.7765\nEpoch 15, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 15, Val Loss: 0.4799, Val Losses Length: 15\nEpoch 15, Train Loss: 0.4677, Val Loss: 0.4799, Val Acc: 0.7765\nEpoch 16, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 16, Val Loss: 0.4769, Val Losses Length: 16\nEpoch 16, Train Loss: 0.4526, Val Loss: 0.4769, Val Acc: 0.7877\nEpoch 17, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 17, Val Loss: 0.4736, Val Losses Length: 17\nEpoch 17, Train Loss: 0.4509, Val Loss: 0.4736, Val Acc: 0.7933\nEpoch 18, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 18, Val Loss: 0.4703, Val Losses Length: 18\nEpoch 18, Train Loss: 0.4460, Val Loss: 0.4703, Val Acc: 0.7933\nEpoch 19, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 19, Val Loss: 0.4668, Val Losses Length: 19\nEpoch 19, Train Loss: 0.4384, Val Loss: 0.4668, Val Acc: 0.7989\nEpoch 20, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 20, Val Loss: 0.4637, Val Losses Length: 20\nEpoch 20, Train Loss: 0.4545, Val Loss: 0.4637, Val Acc: 0.7933\nEpoch 21, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 21, Val Loss: 0.4606, Val Losses Length: 21\nEpoch 21, Train Loss: 0.4324, Val Loss: 0.4606, Val Acc: 0.7989\nEpoch 22, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 22, Val Loss: 0.4579, Val Losses Length: 22\nEpoch 22, Train Loss: 0.4395, Val Loss: 0.4579, Val Acc: 0.7933\nEpoch 23, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 23, Val Loss: 0.4555, Val Losses Length: 23\nEpoch 23, Train Loss: 0.4451, Val Loss: 0.4555, Val Acc: 0.8045\nEpoch 24, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 24, Val Loss: 0.4534, Val Losses Length: 24\nEpoch 24, Train Loss: 0.4362, Val Loss: 0.4534, Val Acc: 0.7989\nEpoch 25, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 25, Val Loss: 0.4516, Val Losses Length: 25\nEpoch 25, Train Loss: 0.4315, Val Loss: 0.4516, Val Acc: 0.7989\nEpoch 26, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 26, Val Loss: 0.4501, Val Losses Length: 26\nEpoch 26, Train Loss: 0.4323, Val Loss: 0.4501, Val Acc: 0.7989\nEpoch 27, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 27, Val Loss: 0.4487, Val Losses Length: 27\nEpoch 27, Train Loss: 0.4279, Val Loss: 0.4487, Val Acc: 0.7989\nEpoch 28, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 28, Val Loss: 0.4475, Val Losses Length: 28\nEpoch 28, Train Loss: 0.4398, Val Loss: 0.4475, Val Acc: 0.7989\nEpoch 29, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 29, Val Loss: 0.4467, Val Losses Length: 29\nEpoch 29, Train Loss: 0.4156, Val Loss: 0.4467, Val Acc: 0.7933\nEpoch 30, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 30, Val Loss: 0.4459, Val Losses Length: 30\nEpoch 30, Train Loss: 0.4345, Val Loss: 0.4459, Val Acc: 0.7933\nEpoch 31, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 31, Val Loss: 0.4455, Val Losses Length: 31\nEpoch 31, Train Loss: 0.4425, Val Loss: 0.4455, Val Acc: 0.7989\nEpoch 32, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 32, Val Loss: 0.4451, Val Losses Length: 32\nEpoch 32, Train Loss: 0.4261, Val Loss: 0.4451, Val Acc: 0.7989\nEpoch 33, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 33, Val Loss: 0.4450, Val Losses Length: 33\nEpoch 33, Train Loss: 0.4221, Val Loss: 0.4450, Val Acc: 0.7989\nEpoch 34, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 34, Val Loss: 0.4451, Val Losses Length: 34\nEpoch 34, Train Loss: 0.4176, Val Loss: 0.4451, Val Acc: 0.7989\nEpoch 35, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 35, Val Loss: 0.4453, Val Losses Length: 35\nEpoch 35, Train Loss: 0.4201, Val Loss: 0.4453, Val Acc: 0.7989\nEpoch 36, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 36, Val Loss: 0.4456, Val Losses Length: 36\nEpoch 36, Train Loss: 0.4250, Val Loss: 0.4456, Val Acc: 0.7989\nEpoch 37, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 37, Val Loss: 0.4459, Val Losses Length: 37\nEpoch 37, Train Loss: 0.4183, Val Loss: 0.4459, Val Acc: 0.8045\nEpoch 38, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 38, Val Loss: 0.4458, Val Losses Length: 38\nEpoch 38, Train Loss: 0.4305, Val Loss: 0.4458, Val Acc: 0.7989\nEpoch 39, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 39, Val Loss: 0.4457, Val Losses Length: 39\nEpoch 39, Train Loss: 0.4173, Val Loss: 0.4457, Val Acc: 0.7989\nEpoch 40, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 40, Val Loss: 0.4456, Val Losses Length: 40\nEpoch 40, Train Loss: 0.4125, Val Loss: 0.4456, Val Acc: 0.7989\nEpoch 41, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 41, Val Loss: 0.4453, Val Losses Length: 41\nEpoch 41, Train Loss: 0.4166, Val Loss: 0.4453, Val Acc: 0.8045\nEpoch 42, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 42, Val Loss: 0.4446, Val Losses Length: 42\nEpoch 42, Train Loss: 0.4211, Val Loss: 0.4446, Val Acc: 0.8101\nEpoch 43, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 43, Val Loss: 0.4437, Val Losses Length: 43\nEpoch 43, Train Loss: 0.4093, Val Loss: 0.4437, Val Acc: 0.8045\nEpoch 44, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 44, Val Loss: 0.4430, Val Losses Length: 44\nEpoch 44, Train Loss: 0.4168, Val Loss: 0.4430, Val Acc: 0.8045\nEpoch 45, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 45, Val Loss: 0.4424, Val Losses Length: 45\nEpoch 45, Train Loss: 0.4143, Val Loss: 0.4424, Val Acc: 0.8045\nEpoch 46, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 46, Val Loss: 0.4418, Val Losses Length: 46\nEpoch 46, Train Loss: 0.4123, Val Loss: 0.4418, Val Acc: 0.8045\nEpoch 47, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 47, Val Loss: 0.4411, Val Losses Length: 47\nEpoch 47, Train Loss: 0.4139, Val Loss: 0.4411, Val Acc: 0.8045\nEpoch 48, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 48, Val Loss: 0.4407, Val Losses Length: 48\nEpoch 48, Train Loss: 0.4204, Val Loss: 0.4407, Val Acc: 0.8045\nEpoch 49, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 49, Val Loss: 0.4405, Val Losses Length: 49\nEpoch 49, Train Loss: 0.4124, Val Loss: 0.4405, Val Acc: 0.8045\nEpoch 50, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 50, Val Loss: 0.4403, Val Losses Length: 50\nEpoch 50, Train Loss: 0.4115, Val Loss: 0.4403, Val Acc: 0.8045\nEpoch 51, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 51, Val Loss: 0.4403, Val Losses Length: 51\nEpoch 51, Train Loss: 0.4110, Val Loss: 0.4403, Val Acc: 0.8045\nEpoch 52, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 52, Val Loss: 0.4405, Val Losses Length: 52\nEpoch 52, Train Loss: 0.4049, Val Loss: 0.4405, Val Acc: 0.8045\nEpoch 53, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 53, Val Loss: 0.4405, Val Losses Length: 53\nEpoch 53, Train Loss: 0.3992, Val Loss: 0.4405, Val Acc: 0.8045\nEpoch 54, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 54, Val Loss: 0.4405, Val Losses Length: 54\nEpoch 54, Train Loss: 0.4102, Val Loss: 0.4405, Val Acc: 0.7933\nEpoch 55, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 55, Val Loss: 0.4403, Val Losses Length: 55\nEpoch 55, Train Loss: 0.4014, Val Loss: 0.4403, Val Acc: 0.7933\nEpoch 56, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 56, Val Loss: 0.4400, Val Losses Length: 56\nEpoch 56, Train Loss: 0.4006, Val Loss: 0.4400, Val Acc: 0.7933\nEpoch 57, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 57, Val Loss: 0.4396, Val Losses Length: 57\nEpoch 57, Train Loss: 0.4082, Val Loss: 0.4396, Val Acc: 0.7877\nEpoch 58, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 58, Val Loss: 0.4395, Val Losses Length: 58\nEpoch 58, Train Loss: 0.4041, Val Loss: 0.4395, Val Acc: 0.7933\nEpoch 59, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 59, Val Loss: 0.4392, Val Losses Length: 59\nEpoch 59, Train Loss: 0.3998, Val Loss: 0.4392, Val Acc: 0.7933\nEpoch 60, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 60, Val Loss: 0.4388, Val Losses Length: 60\nEpoch 60, Train Loss: 0.4064, Val Loss: 0.4388, Val Acc: 0.7933\nEpoch 61, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 61, Val Loss: 0.4384, Val Losses Length: 61\nEpoch 61, Train Loss: 0.4000, Val Loss: 0.4384, Val Acc: 0.7933\nEpoch 62, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 62, Val Loss: 0.4384, Val Losses Length: 62\nEpoch 62, Train Loss: 0.3939, Val Loss: 0.4384, Val Acc: 0.7989\nEpoch 63, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 63, Val Loss: 0.4389, Val Losses Length: 63\nEpoch 63, Train Loss: 0.3990, Val Loss: 0.4389, Val Acc: 0.8045\nEpoch 64, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 64, Val Loss: 0.4395, Val Losses Length: 64\nEpoch 64, Train Loss: 0.4108, Val Loss: 0.4395, Val Acc: 0.8045\nEpoch 65, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 65, Val Loss: 0.4400, Val Losses Length: 65\nEpoch 65, Train Loss: 0.4024, Val Loss: 0.4400, Val Acc: 0.8045\nEpoch 66, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 66, Val Loss: 0.4400, Val Losses Length: 66\nEpoch 66, Train Loss: 0.4140, Val Loss: 0.4400, Val Acc: 0.8045\nEpoch 67, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 67, Val Loss: 0.4399, Val Losses Length: 67\nEpoch 67, Train Loss: 0.4080, Val Loss: 0.4399, Val Acc: 0.8045\nEpoch 68, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 68, Val Loss: 0.4398, Val Losses Length: 68\nEpoch 68, Train Loss: 0.4012, Val Loss: 0.4398, Val Acc: 0.7989\nEpoch 69, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 69, Val Loss: 0.4396, Val Losses Length: 69\nEpoch 69, Train Loss: 0.4024, Val Loss: 0.4396, Val Acc: 0.7933\nEpoch 70, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 70, Val Loss: 0.4392, Val Losses Length: 70\nEpoch 70, Train Loss: 0.3966, Val Loss: 0.4392, Val Acc: 0.7933\nEpoch 71, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 71, Val Loss: 0.4391, Val Losses Length: 71\nEpoch 71, Train Loss: 0.3978, Val Loss: 0.4391, Val Acc: 0.7989\nEpoch 72, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 72, Val Loss: 0.4389, Val Losses Length: 72\nEpoch 72, Train Loss: 0.4013, Val Loss: 0.4389, Val Acc: 0.7989\nEpoch 73, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 73, Val Loss: 0.4388, Val Losses Length: 73\nEpoch 73, Train Loss: 0.4048, Val Loss: 0.4388, Val Acc: 0.7989\nEpoch 74, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 74, Val Loss: 0.4390, Val Losses Length: 74\nEpoch 74, Train Loss: 0.3943, Val Loss: 0.4390, Val Acc: 0.7989\nEpoch 75, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 75, Val Loss: 0.4389, Val Losses Length: 75\nEpoch 75, Train Loss: 0.4009, Val Loss: 0.4389, Val Acc: 0.7989\nEpoch 76, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 76, Val Loss: 0.4387, Val Losses Length: 76\nEpoch 76, Train Loss: 0.3864, Val Loss: 0.4387, Val Acc: 0.7989\nEpoch 77, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 77, Val Loss: 0.4384, Val Losses Length: 77\nEpoch 77, Train Loss: 0.3985, Val Loss: 0.4384, Val Acc: 0.8045\nEpoch 78, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 78, Val Loss: 0.4379, Val Losses Length: 78\nEpoch 78, Train Loss: 0.4058, Val Loss: 0.4379, Val Acc: 0.8045\nEpoch 79, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 79, Val Loss: 0.4375, Val Losses Length: 79\nEpoch 79, Train Loss: 0.4086, Val Loss: 0.4375, Val Acc: 0.7989\nEpoch 80, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 80, Val Loss: 0.4371, Val Losses Length: 80\nEpoch 80, Train Loss: 0.3874, Val Loss: 0.4371, Val Acc: 0.7989\nEpoch 81, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 81, Val Loss: 0.4366, Val Losses Length: 81\nEpoch 81, Train Loss: 0.3940, Val Loss: 0.4366, Val Acc: 0.7989\nEpoch 82, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 82, Val Loss: 0.4360, Val Losses Length: 82\nEpoch 82, Train Loss: 0.4004, Val Loss: 0.4360, Val Acc: 0.7989\nEpoch 83, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 83, Val Loss: 0.4355, Val Losses Length: 83\nEpoch 83, Train Loss: 0.4012, Val Loss: 0.4355, Val Acc: 0.7933\nEpoch 84, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 84, Val Loss: 0.4355, Val Losses Length: 84\nEpoch 84, Train Loss: 0.3936, Val Loss: 0.4355, Val Acc: 0.7989\nEpoch 85, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 85, Val Loss: 0.4358, Val Losses Length: 85\nEpoch 85, Train Loss: 0.3999, Val Loss: 0.4358, Val Acc: 0.7877\nEpoch 86, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 86, Val Loss: 0.4367, Val Losses Length: 86\nEpoch 86, Train Loss: 0.3886, Val Loss: 0.4367, Val Acc: 0.7933\nEpoch 87, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 87, Val Loss: 0.4378, Val Losses Length: 87\nEpoch 87, Train Loss: 0.3884, Val Loss: 0.4378, Val Acc: 0.7933\nEpoch 88, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 88, Val Loss: 0.4389, Val Losses Length: 88\nEpoch 88, Train Loss: 0.3917, Val Loss: 0.4389, Val Acc: 0.7877\nEpoch 89, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 89, Val Loss: 0.4398, Val Losses Length: 89\nEpoch 89, Train Loss: 0.4010, Val Loss: 0.4398, Val Acc: 0.7877\nEpoch 90, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 90, Val Loss: 0.4398, Val Losses Length: 90\nEpoch 90, Train Loss: 0.3954, Val Loss: 0.4398, Val Acc: 0.7821\nEpoch 91, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 91, Val Loss: 0.4391, Val Losses Length: 91\nEpoch 91, Train Loss: 0.3880, Val Loss: 0.4391, Val Acc: 0.7821\nEpoch 92, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 92, Val Loss: 0.4378, Val Losses Length: 92\nEpoch 92, Train Loss: 0.3961, Val Loss: 0.4378, Val Acc: 0.7933\nEpoch 93, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 93, Val Loss: 0.4367, Val Losses Length: 93\nEpoch 93, Train Loss: 0.3924, Val Loss: 0.4367, Val Acc: 0.7933\nEpoch 94, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 94, Val Loss: 0.4354, Val Losses Length: 94\nEpoch 94, Train Loss: 0.3935, Val Loss: 0.4354, Val Acc: 0.7989\nEpoch 95, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 95, Val Loss: 0.4344, Val Losses Length: 95\nEpoch 95, Train Loss: 0.3914, Val Loss: 0.4344, Val Acc: 0.8156\nEpoch 96, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 96, Val Loss: 0.4340, Val Losses Length: 96\nEpoch 96, Train Loss: 0.3971, Val Loss: 0.4340, Val Acc: 0.8045\nEpoch 97, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 97, Val Loss: 0.4338, Val Losses Length: 97\nEpoch 97, Train Loss: 0.3894, Val Loss: 0.4338, Val Acc: 0.8045\nEpoch 98, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 98, Val Loss: 0.4340, Val Losses Length: 98\nEpoch 98, Train Loss: 0.3955, Val Loss: 0.4340, Val Acc: 0.8101\nEpoch 99, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 99, Val Loss: 0.4343, Val Losses Length: 99\nEpoch 99, Train Loss: 0.3883, Val Loss: 0.4343, Val Acc: 0.8156\nEpoch 100, val_out shape: torch.Size([891, 2]), val_mask shape: torch.Size([891]), y shape: torch.Size([891])\nEpoch 100, Val Loss: 0.4349, Val Losses Length: 100\nEpoch 100, Train Loss: 0.3861, Val Loss: 0.4349, Val Acc: 0.8045\n","output_type":"stream"}],"execution_count":314},{"cell_type":"code","source":"print(f\"Validation block executed {val_block_count} times\")\nprint(f\"Length of train_losses: {len(train_losses)}, Length of val_losses: {len(val_losses)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:37.455263Z","iopub.execute_input":"2025-05-11T23:45:37.455567Z","iopub.status.idle":"2025-05-11T23:45:37.459721Z","shell.execute_reply.started":"2025-05-11T23:45:37.455540Z","shell.execute_reply":"2025-05-11T23:45:37.459215Z"}},"outputs":[{"name":"stdout","text":"Validation block executed 100 times\nLength of train_losses: 100, Length of val_losses: 100\n","output_type":"stream"}],"execution_count":315},{"cell_type":"markdown","source":"| Part | Purpose | Key Functionality | Biostatistical/ML Relevance |\n|---|---|---|---|\n| Install Dependencies | Installs torch-geometric | Runs `!pip install` | Enables GNN implementation |\n| Import Libraries | Loads required tools | Imports pandas, torch, etc. | Supports data processing and modeling |\n| Load Dataset | Reads train.csv | Uses `pd.read_csv` | Provides raw data for analysis |\n| Handle Missing Values | Imputes missing Age, Embarked | Fills with mean/mode | Ensures complete data for modeling |\n| Encode Categorical Variables | Converts Sex, Embarked, Pclass to numerical | Uses mapping and one-hot encoding | Prepares data for ML |\n| Drop Irrelevant Columns | Removes PassengerId, Name, etc. | Uses `drop` | Reduces noise |\n| Define Features/Target | Separates X and y | Uses `drop` | Sets up supervised learning |\n| Standardize Features | Scales features | Uses StandardScaler | Ensures fair distance calculations |\n| Split Train/Validation | Creates training and validation sets | Uses `train_test_split` | Enables model evaluation |\n| Construct Graph | Builds k-NN graph | Computes distances, creates edges | Enables relational modeling |\n| Create Data Object | Prepares data for GNN | Uses Data | Integrates features, edges, labels |\n| Define GCN Model | Specifies GCN architecture | Uses GCNConv layers | Implements graph-based learning |\n| Initialize Model/Optimizer | Sets up training | Moves to GPU, configures Adam | Prepares for optimization |\n| Training Loop | Trains the model | Computes loss, updates weights | Optimizes survival predictions |\n| Validation | Evaluates on validation set | Computes loss, accuracy | Assesses generalization |\n","metadata":{}},{"cell_type":"markdown","source":"## 16.Save the state dictionary and Save the entire model","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/titanic_gcn_model.pth')\ntorch.save(model, '/kaggle/working/titanic_gcn_model.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:37.460372Z","iopub.execute_input":"2025-05-11T23:45:37.460598Z","iopub.status.idle":"2025-05-11T23:45:37.480640Z","shell.execute_reply.started":"2025-05-11T23:45:37.460574Z","shell.execute_reply":"2025-05-11T23:45:37.480121Z"}},"outputs":[],"execution_count":316},{"cell_type":"markdown","source":"## 17. Visualization and Metrics Setup\n- Functionality:\n    - <span style=\"color:orange\">model.eval()</span>: Switches the model to evaluation mode, disabling dropout and batch normalization layers used during training to ensure consistent predictions.\n    - <span style=\"color:orange\">with torch.no_grad()</span>: Disables gradient computation to save memory and speed up inference, as no backpropagation is needed for evaluation.\n    - <span style=\"color:orange\">val_out = model(data)</span>: Runs the model on the entire dataset to get output logits (raw scores) for all nodes in the graph.\n    - <span style=\"color:orange\">_, pred = val_out.max(dim=1)</span>: Extracts predicted classes by selecting the index (0 or 1) with the highest logit score along dimension 1 (class dimension).\n    - <span style=\"color:orange\">val_pred = pred[data.val_mask].cpu().numpy()</span>: Filters predictions for the validation set (using val_mask), moves them to CPU, and converts to a NumPy array for compatibility with scikit-learn metrics.\n    - <span style=\"color:orange\">val_true = data.y[data.val_mask].cpu().numpy()</span>: Gets the true labels for the validation set, similarly converted to NumPy.\n    - <span style=\"color:orange\">val_probs = torch.softmax(val_out[data.val_mask], dim=1)[:, 1].cpu().numpy()</span>: Applies softmax to convert validation logits to probabilities, selects the probability for class 1 (Survived), and converts to NumPy.\n<br>\n<br>\n- This block prepares the data needed for performance metrics and visualizations. It generates predictions (<span style=\"color:orange\">val_pred</span>), true labels (<span style=\"color:orange\">val_true</span>), and probabilities (<span style=\"color:orange\">val_probs</span>) for the validation set, which are used in subsequent metrics (F1 score, confusion matrix, ROC curve).\n- For the Titanic dataset, where the goal is to predict survival (binary classification), these outputs allow you to evaluate how well the GCN model distinguishes between \"Survived\" (1) and \"Not Survived\" (0).\n- The use of <span style=\"color:orange\">val_mask</span> ensures only the validation subset (20% of the data, as defined in your train-test split) is evaluated, avoiding data leakage and providing an unbiased estimate of performance.","metadata":{}},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    val_out = model(data)\n    _, pred = val_out.max(dim=1)\n    val_pred = pred[data.val_mask].cpu().numpy()\n    val_true = data.y[data.val_mask].cpu().numpy()\n    val_probs = torch.softmax(val_out[data.val_mask], dim=1)[:, 1].cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:37.481287Z","iopub.execute_input":"2025-05-11T23:45:37.481451Z","iopub.status.idle":"2025-05-11T23:45:37.503246Z","shell.execute_reply.started":"2025-05-11T23:45:37.481437Z","shell.execute_reply":"2025-05-11T23:45:37.502702Z"}},"outputs":[],"execution_count":317},{"cell_type":"markdown","source":"## 18. F1 Score\n- <span style=\"color:orange\">f1_score(val_true, val_pred)</span>: Computes the F1 score using scikit-learn, which is the harmonic mean of precision and recall, defined as: F1 = 2 * (precision + recall) / (precision * recall)\n- <span style=\"color:orange\">val_true</span>: True validation labels (0 or 1).\n- <span style=\"color:orange\">val_pred</span>: Predicted validation labels (0 or 1).\n- <span style=\"color:orange\">print(f'Validation F1 Score: {f1:.4f}')</span>: Outputs the F1 score to the console, formatted to 4 decimal places.\n\nThe F1 score is particularly relevant for the Titanic dataset, which may have class imbalance (more passengers did not survive than survived). Unlike accuracy, F1 balances precision (correct positive predictions) and recall (capturing all positive cases), making it robust for evaluating performance on minority classes (e.g., survivors).\n\n","metadata":{}},{"cell_type":"code","source":"f1 = f1_score(val_true, val_pred)\nprint(f'Validation F1 Score: {f1:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:37.503949Z","iopub.execute_input":"2025-05-11T23:45:37.504172Z","iopub.status.idle":"2025-05-11T23:45:37.526262Z","shell.execute_reply.started":"2025-05-11T23:45:37.504150Z","shell.execute_reply":"2025-05-11T23:45:37.525598Z"}},"outputs":[{"name":"stdout","text":"Validation F1 Score: 0.7107\n","output_type":"stream"}],"execution_count":318},{"cell_type":"markdown","source":"## 19. Confusion Matrix\n- Functionality:\n    - <span style=\"color:orange\">confusion_matrix(val_true, val_pred)</span>: Computes a 2x2 matrix where rows represent true labels and columns represent predicted labels. For binary classification:\n        - <span style=\"color:orange\">cm[0,0]</span>: True Negatives (correctly predicted Not Survived).\n        - <span style=\"color:orange\">cm[0,1]</span>: False Positives (incorrectly predicted Survived).\n        - <span style=\"color:orange\">cm[1,0]</span>: False Negatives (incorrectly predicted Not Survived).\n        - <span style=\"color:orange\">cm[1,1]</span>: True Positives (correctly predicted Survived).\n    - <span style=\"color:orange\">plt.figure(figsize=(8, 6))</span>: Creates a plot with dimensions 8x6 inches.\n    - <span style=\"color:orange\">sns.heatmap(...)</span>: Uses seaborn to plot the confusion matrix as a heatmap:\n        - <span style=\"color:orange\">annot=True</span>: Displays the count in each cell.\n        - <span style=\"color:orange\">fmt='d'</span>: Formats numbers as integers.\n        - <span style=\"color:orange\">cmap='Blues'</span>: Uses a blue color scheme for visual clarity.\n        - <span style=\"color:orange\">xticklabels/yticklabels</span>: Labels axes as \"Not Survived\" and \"Survived\" for interpretability.\n    - <span style=\"color:orange\">plt.title, plt.xlabel, plt.ylabel</span>: Sets the title and axis labels.\n    - <span style=\"color:orange\">plt.savefig('/kaggle/working/confusion_matrix.png')</span>: Saves the plot to Kaggle’s output directory.\n    - <span style=\"color:orange\">plt.close()</span>: Closes the plot to free memory and prevent display in Kaggle (non-interactive environment).\n<br><br>\n- The confusion matrix directly shows the model’s classification errors, which is critical for biostatistics tasks like survival prediction. For example, false negatives (failing to predict survival) could be more costly than false positives in a real-world context.\n- For the Titanic dataset, it helps you see if the model is biased toward predicting \"Not Survived\" due to class imbalance, allowing you to assess whether it’s capturing the minority class (Survived) effectively.","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(val_true, val_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.savefig('/kaggle/working/confusion_matrix.png')\nplt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:37.526889Z","iopub.execute_input":"2025-05-11T23:45:37.527093Z","iopub.status.idle":"2025-05-11T23:45:37.683139Z","shell.execute_reply.started":"2025-05-11T23:45:37.527057Z","shell.execute_reply":"2025-05-11T23:45:37.682314Z"}},"outputs":[],"execution_count":319},{"cell_type":"markdown","source":"## 20. Training and Validation Loss Curves\n- Functionality:\n    - <span style=\"color:orange\">plt.figure(figsize=(10, 6))</span>: Creates a plot with dimensions 10x6 inches.\n    - <span style=\"color:orange\">plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')</span>: Plots the training loss (train_losses, collected during the training loop) against epoch numbers (1 to 100).\n    - <span style=\"color:orange\">plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')</span>: Plots the validation loss (<span style=\"color:orange\">val_losses</span>, collected during the training loop) against epochs.\n    - <span style=\"color:orange\">plt.title, plt.xlabel, plt.ylabel</span>: Sets the title and axis labels.\n    - <span style=\"color:orange\">plt.legend()</span>: Adds a legend to distinguish training and validation curves.\n    - <span style=\"color:orange\">plt.grid(True)</span>: Adds a grid for better readability.\n    - <span style=\"color:orange\">plt.savefig('/kaggle/working/loss_curves.png')</span>: Saves the plot to Kaggle’s output directory.\n    - <span style=\"color:orange\">plt.close()</span>: Closes the plot to free memory.\n<br><br>\n- Loss curves show how the model’s error (negative log-likelihood loss in your case) decreases over time for both training and validation sets. This is crucial for diagnosing model behavior:\n    - If training loss decreases but validation loss plateaus or increases, the model is overfitting.\n    - If both losses are high, the model is underfitting.\n- For the Titanic dataset, this helps you determine if the GCN is learning meaningful patterns in the graph structure or if it’s memorizing the training data.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\nplt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\nplt.title('Training and Validation Loss Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.savefig('/kaggle/working/loss_curves.png')\nplt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:45:37.683930Z","iopub.execute_input":"2025-05-11T23:45:37.684219Z","iopub.status.idle":"2025-05-11T23:45:37.826393Z","shell.execute_reply.started":"2025-05-11T23:45:37.684197Z","shell.execute_reply":"2025-05-11T23:45:37.825835Z"}},"outputs":[],"execution_count":320},{"cell_type":"markdown","source":"## 21. ROC Curve\n- Functionality:\n    - <span style=\"color:orange\">roc_curve(val_true, val_probs)</span>: Computes the Receiver Operating Characteristic (ROC) curve, returning:\n        - <span style=\"color:orange\">fpr</span>: False Positive Rate (FPR = False Positives / (False Positives + True Negatives))\n        - <span style=\"color:orange\">tpr</span>: True Positive Rate (TPR = True Positives / (True Positives + False Negatives), also called recall).\n    - <span style=\"color:orange\">_</span>: Thresholds, ignored here.\n    - <span style=\"color:orange\">val_probs</span>: Probabilities for class 1 (Survived), used to evaluate performance at different classification thresholds.\n    - <span style=\"color:orange\">roc_auc = auc(fpr, tpr)</span>: Computes the Area Under the Curve (AUC) using the trapezoidal rule, quantifying the ROC curve’s quality.\n    - <span style=\"color:orange\">plt.figure(figsize=(8, 6))</span>: Creates a plot with dimensions 8x6 inches.\n    - <span style=\"color:orange\">plt.plot(fpr, tpr, ...)</span>: Plots the ROC curve with:\n        - <span style=\"color:orange\">color='darkorange'</span>: Orange color for visibility.\n        - <span style=\"color:orange\">lw=2</span>: Line width of 2.\n        - <span style=\"color:orange\">label=f'ROC curve (AUC = {roc_auc:.2f})'</span>: Labels the curve with the AUC value.\n    - <span style=\"color:orange\">plt.plot([0, 1], [0, 1], ...)</span>: Plots a diagonal dashed line (random classifier baseline, AUC = 0.5).\n    - <span style=\"color:orange\">plt.xlim, plt.ylim</span>: Sets axis limits for clarity.\n    - <span style=\"color:orange\">plt.xlabel, plt.ylabel, plt.title</span>: Sets axis labels and title.\n    - <span style=\"color:orange\">plt.legend(loc='lower right')</span>: Places the legend in the lower-right corner.\n    - <span style=\"color:orange\">plt.grid(True)</span>: Adds a grid.\n    - <span style=\"color:orange\">plt.savefig('/kaggle/working/roc_curve.png')</span>: Saves the plot to Kaggle’s output directory.\n    - <span style=\"color:orange\">plt.close()</span>: Closes the plot.\n<br><br>\n- The ROC curve evaluates the model’s ability to distinguish between classes (Survived vs. Not Survived) across all possible thresholds. A higher AUC (closer to 1) indicates better performance; AUC = 0.5 suggests random guessing.\n- For the Titanic dataset, where survival prediction involves imbalanced classes, the ROC curve helps assess how well the model balances sensitivity (detecting survivors) and specificity (avoiding false positives).","metadata":{}},{"cell_type":"code","source":"fpr, tpr, _ = roc_curve(val_true, val_probs)\nroc_auc = auc(fpr, tpr)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.savefig('/kaggle/working/roc_curve.png')\nplt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:46:04.443338Z","iopub.execute_input":"2025-05-11T23:46:04.443941Z","iopub.status.idle":"2025-05-11T23:46:04.572334Z","shell.execute_reply.started":"2025-05-11T23:46:04.443919Z","shell.execute_reply":"2025-05-11T23:46:04.571569Z"}},"outputs":[],"execution_count":322}]}